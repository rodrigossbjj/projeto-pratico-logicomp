{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Comparação de resultados: LLM x Z3\nEste notebook consolida os registros salvos pelo pipeline e apresenta uma visão rápida de como cada modelo se comporta ao resolver os puzzles Knights and Knaves em comparação com a solução obtida via Z3."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Como usar\n1. Gere puzzles e execute `main.py`, `main_gemini.py` ou `main_gpt.py` para popular os arquivos `resultados/*.jsonl`.\n2. Reabra ou reexecute este notebook para atualizar as métricas e gráficos."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import json\nfrom pathlib import Path\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-v0_8-muted')\npd.options.display.max_colwidth = 120\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def load_jsonl(path: str, model_label: str) -> pd.DataFrame:\n    file_path = Path(path)\n    if not file_path.exists():\n        print(f\"Aviso: arquivo {path} não encontrado.\")\n        return pd.DataFrame()\n\n    rows = []\n    with file_path.open(encoding='utf-8') as fp:\n        for line in fp:\n            line = line.strip()\n            if not line:\n                continue\n            rows.append(json.loads(line))\n\n    if not rows:\n        print(f\"Aviso: arquivo {path} está vazio.\")\n        return pd.DataFrame()\n\n    df = pd.DataFrame(rows)\n    if 'model' in df.columns:\n        df['model_name'] = df['model']\n    else:\n        df['model_name'] = model_label\n\n    df['match'] = df['match'].astype(bool)\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    return df\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "gemini_df = load_jsonl('resultados/results.jsonl', 'gemini-2.5-flash')\ngpt_df = load_jsonl('resultados/results_gpt.jsonl', 'gpt-4o-mini')\n\ndf = pd.concat([gemini_df, gpt_df], ignore_index=True)\ndf = df.sort_values('timestamp') if not df.empty else df\ndf.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visão geral por modelo"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if df.empty:\n    print('Sem dados disponíveis. Execute os scripts principais antes de rodar este notebook.')\nelse:\n    resumo = (\n        df.groupby('model_name')['match']\n        .agg(total='count', acertos='sum')\n        .assign(taxa_acerto=lambda x: (x['acertos'] / x['total'] * 100).round(1))\n        .sort_values('taxa_acerto', ascending=False)\n    )\n    display(resumo)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if not df.empty:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    resumo.reset_index().plot(\n        x='model_name', y='taxa_acerto', kind='bar', color='#4c72b0', legend=False, ax=ax\n    )\n    ax.set_ylabel('Taxa de acerto (%)')\n    ax.set_xlabel('Modelo')\n    ax.set_ylim(0, 100)\n    ax.set_title('Precisão por modelo')\n    for idx, value in enumerate(resumo['taxa_acerto']):\n        ax.text(idx, value + 1, f\"{value:.1f}%\", ha='center')\n    plt.tight_layout()\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Desempenho por puzzle"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if not df.empty:\n    puzzle_summary = (\n        df.groupby(['puzzle_file', 'model_name'])['match']\n        .agg(total='count', acertos='sum')\n        .assign(taxa_acerto=lambda x: (x['acertos'] / x['total'] * 100).round(1))\n        .reset_index()\n        .sort_values(['puzzle_file', 'model_name'])\n    )\n    display(puzzle_summary)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Onde os modelos erraram"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if not df.empty:\n    erros = df[~df['match']]\n    if erros.empty:\n        print('Nenhum erro registrado até o momento.')\n    else:\n        display(\n            erros[\n                ['timestamp', 'model_name', 'puzzle_file', 'llm_answer', 'z3_answer']\n            ].sort_values('timestamp')\n        )\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Pronto! Sempre que gerar novos resultados, reexecute todas as células para atualizar as tabelas e gráficos."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}